{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4da983-17d9-4827-9f5a-a1c63d6b165a",
   "metadata": {},
   "source": [
    "# Reflection Service for Toxicity Reduction\n",
    "\n",
    "**NOTE**: This is adapted from the original notebook in the [core llama-index repo](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/agent/llama-index-agent-introspective/examples/toxicity_reduction.ipynb).\n",
    "\n",
    "In this notebook, we cover how to setup a reflection service that can perform toxicity reflection and correction.\n",
    "\n",
    "We make use of two types of reflection services as \"agents\" in llama-agents: \n",
    "\n",
    "- A self-reflection agent that can reflect and correct a given response without any external tools\n",
    "- A CRITIC agent that can reflect and correct a given response using external tools.\n",
    "\n",
    "We set these up as **independent** services, meaning they don't communicate. The purpose of this notebook is to show you how to convert a reflection agent into a service that you can interact with.\n",
    "\n",
    "In this notebook we make use of our prepackaged reflection agents using our `llama-index-agent-introspective` LlamaPack. This is primarily for concision.\n",
    "\n",
    "*However*, if you wish to build reflection from scratch we highly encourage you to do so! All LlamaPacks from LlamaHub can and should be downloaded locally, and directly inspected/modified as code files. This is highly encouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9bad33-e2d9-4b79-84d1-ffa82683f434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-agent-introspective -q\n",
    "%pip install google-api-python-client -q\n",
    "%pip install llama-index-llms-openai -q\n",
    "%pip install llama-index-program-openai -q\n",
    "%pip install llama-index-readers-file -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbccfe5e-c25f-469a-9394-d7dffcdaae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6f019-facf-4860-8781-87cf85d4beb8",
   "metadata": {},
   "source": [
    "## 1 Toxicity Reduction: Problem Setup\n",
    "\n",
    "In this notebook, the task we'll have our introspective agents perform is \"toxicity reduction\". In particular, given a certain harmful text we'll ask the agent to produce a less harmful (or more safe) version of the original text. As mentioned before, our introspective agent will do this by performing reflection and correction cycles until reaching an adequately safe version of the toxic text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84169405-5fd7-4133-b8e6-5c52deaef775",
   "metadata": {},
   "source": [
    "### 1.a Setup our CRITIC Agent\n",
    "\n",
    "Our CRITIC Agent makes use of an external tool to reflect/validate the response, and then correct it. We will use our prepackaged `ToolInteractiveReflectiveAgent` for this purpose.\n",
    "\n",
    "The CRITIC agent delegates the critique subtask to a `CritiqueAgentWorker`, and then performs correction with a standalone LLM call.\n",
    "\n",
    "The first thing we will do here is define the `PerspectiveTool`, which our `ToolInteractiveReflectionAgent` will make use of through another agent, namely a `CritiqueAgent`.\n",
    "\n",
    "To use Perspective's API, you will need to do the following steps:\n",
    "\n",
    "1. Enable the Perspective API in your Google Cloud projects\n",
    "2. Generate a new set of credentials (i.e. API key) that you will need to either set an env var `PERSPECTIVE_API_KEY` or supply directly in the appropriate parts of the code that follows.\n",
    "\n",
    "To perform steps 1. and 2., you can follow the instructions outlined here: https://developers.perspectiveapi.com/s/docs-enable-the-api?language=en_US."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc789c-41f0-4f0b-b1ef-17e9bb180132",
   "metadata": {},
   "source": [
    "#### Build `PerspectiveTool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1a00dd-17bb-4227-90ca-d544cbd16c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from typing import Dict, Optional\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Perspective:\n",
    "    \"\"\"Custom class to interact with Perspective API.\"\"\"\n",
    "\n",
    "    attributes = [\n",
    "        \"toxicity\",\n",
    "        \"severe_toxicity\",\n",
    "        \"identity_attack\",\n",
    "        \"insult\",\n",
    "        \"profanity\",\n",
    "        \"threat\",\n",
    "        \"sexually_explicit\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None) -> None:\n",
    "        if api_key is None:\n",
    "            try:\n",
    "                api_key = os.environ[\"PERSPECTIVE_API_KEY\"]\n",
    "            except KeyError:\n",
    "                raise ValueError(\n",
    "                    \"Please provide an api key or set PERSPECTIVE_API_KEY env var.\"\n",
    "                )\n",
    "\n",
    "        self._client = discovery.build(\n",
    "            \"commentanalyzer\",\n",
    "            \"v1alpha1\",\n",
    "            developerKey=api_key,\n",
    "            discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "            static_discovery=False,\n",
    "        )\n",
    "\n",
    "    def get_toxicity_scores(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Function that makes API call to Perspective to get toxicity scores across various attributes.\"\"\"\n",
    "\n",
    "        analyze_request = {\n",
    "            \"comment\": {\"text\": text},\n",
    "            \"requestedAttributes\": {att.upper(): {} for att in self.attributes},\n",
    "        }\n",
    "\n",
    "        response = self._client.comments().analyze(body=analyze_request).execute()\n",
    "        try:\n",
    "            return {\n",
    "                att: response[\"attributeScores\"][att.upper()][\"summaryScore\"][\"value\"]\n",
    "                for att in self.attributes\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Unable to parse response\") from e\n",
    "\n",
    "\n",
    "perspective = Perspective()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45794e7-8175-4a6c-a9ae-bfe76274e755",
   "metadata": {},
   "source": [
    "With the helper class in hand, we can define our tool by first defining a function and then making use of the `FunctionTool` abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7c1518-153a-4e2d-8620-241d1a547fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from llama_index.core.bridge.pydantic import Field\n",
    "\n",
    "\n",
    "def perspective_function_tool(\n",
    "    text: str = Field(\n",
    "        default_factory=str, description=\"The text to compute toxicity scores on.\"\n",
    "    )\n",
    ") -> Tuple[str, float]:\n",
    "    \"\"\"Returns the toxicity score of the most problematic toxic attribute.\"\"\"\n",
    "\n",
    "    scores = perspective.get_toxicity_scores(text=text)\n",
    "    max_key = max(scores, key=scores.get)\n",
    "    return (max_key, scores[max_key] * 100)\n",
    "\n",
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "pespective_tool = FunctionTool.from_defaults(\n",
    "    perspective_function_tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d44115-1cb3-4cd7-90e8-bb4fc907a22e",
   "metadata": {},
   "source": [
    "A simple test of our perspective tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f45fde-25bc-4fae-bd24-791cccd58152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('toxicity', 2.6028076)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspective_function_tool(text=\"friendly greetings from python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5abb0a-f119-4c98-8a62-96465d21a200",
   "metadata": {},
   "source": [
    "#### Build a stateful agent function with `ToolInteractiveReflectionAgent`\n",
    "\n",
    "We define a stateful agent function that wraps the prepackaged `ToolInteractiveReflectionAgent`. This stateful agent function will then be directly turned into a service.\n",
    "\n",
    "**NOTE**: This CRITIC agent is using `ToolInteractiveReflectionAgent` out of convenience (which is why it looks like an agent wrapping another agent). If you're building reflection from scratch we highly encourage you to define your own custom stateful agent function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84daa3d1-075c-4740-ac03-efe077f2151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.introspective import IntrospectiveAgentWorker\n",
    "from llama_index.agent.introspective import ToolInteractiveReflectionAgentWorker\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.agent.openai import OpenAIAgentWorker\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "\n",
    "def get_tool_interactive_reflection_agent(verbose: bool = True):\n",
    "    \"\"\"Helper function for getting the tool-interactive reflection agent.\n",
    "\n",
    "    Steps:\n",
    "\n",
    "    Define the `ToolInteractiveReflectionAgentWorker`\n",
    "    - Construct a CritiqueAgentWorker that performs reflection with tools.\n",
    "    - Define an LLM that will be used to generate corrections against the critique.\n",
    "    - Define a function that determines the stopping condition for reflection/correction\n",
    "            cycles\n",
    "    - Construct `ToolInteractiveReflectionAgentWorker` using .from_defaults()\n",
    "    \"\"\"\n",
    "    critique_agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "        tools=[pespective_tool], llm=OpenAI(\"gpt-3.5-turbo\"), verbose=verbose\n",
    "    )\n",
    "    correction_llm = OpenAI(\"gpt-4-turbo-preview\")\n",
    "    def stopping_callable(critique_str: str) -> bool:\n",
    "        \"\"\"Function that determines stopping condition for reflection & correction cycles.\n",
    "\n",
    "        critique_str [str]: The response string provided by the critique agent.\n",
    "        \"\"\"\n",
    "\n",
    "        return \"[PASS]\" in critique_str\n",
    "\n",
    "    \n",
    "    \n",
    "    agent_worker = ToolInteractiveReflectionAgentWorker.from_defaults(\n",
    "        critique_agent_worker=critique_agent_worker,\n",
    "        critique_template=(\n",
    "            \"Please provide a toxicity critique for the provided text. \"\n",
    "            \"If the toxicity score is less than 3.0 then please \"\n",
    "            \"write '[PASS]' otherwise write '[FAIL]'. \"\n",
    "            \"Here is the text:\\n {input_str}\"\n",
    "        ),\n",
    "        stopping_callable=stopping_callable,\n",
    "        correction_llm=correction_llm,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    chat_history = [\n",
    "        ChatMessage(\n",
    "            content=\"You are an assistant that generates safer versions of potentially toxic, user-supplied text.\",\n",
    "            role=MessageRole.SYSTEM,\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return agent_worker.as_agent(chat_history=chat_history)\n",
    "\n",
    "\n",
    "critic_agent_prepackaged = get_tool_interactive_reflection_agent(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83bb84c9-a100-47e1-b4d3-943c22fc8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap with stateful function\n",
    "from llama_index.core.agent import FnAgentWorker\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "def critic_agent_fn(state: Dict[str, Any]) -> Tuple[Dict[str, Any], bool]:\n",
    "    \"\"\"Critic agent function.\"\"\"\n",
    "    critic_agent_prepackaged, input_str = state[\"critic_agent_prepackaged\"], state[\"__task__\"].input\n",
    "    response = critic_agent_prepackaged.query(input_str)\n",
    "    return str(response), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d063790d-2e70-4b7a-a9ab-48e0ecc5c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_agent = FnAgentWorker(\n",
    "    fn=critic_agent_fn, initial_state={\n",
    "        \"critic_agent_prepackaged\": critic_agent_prepackaged, \n",
    "    }\n",
    ").as_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56403a49-2de6-40d1-a1f3-a312b05aa612",
   "metadata": {},
   "source": [
    "### 1.b Setup our Self-Reflection Agent\n",
    "\n",
    "Similar to the previous subsection, we now define a self-reflection agent using our prepackaged `SelfReflectionAgentWorker` LlamaPack module. This reflection technique doesn't make use of any tools, and instead only uses a supplied LLM to perform both reflection and correction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74119d00-26d8-4c6a-aadc-c7af6c0271cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.introspective import SelfReflectionAgentWorker\n",
    "\n",
    "\n",
    "def get_self_reflection_agent(verbose: bool = True):\n",
    "    \"\"\"Helper function for building a self reflection agent.\"\"\"\n",
    "\n",
    "    self_reflection_agent_worker = SelfReflectionAgentWorker.from_defaults(\n",
    "        llm=OpenAI(\"gpt-4-turbo-preview\"),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    chat_history = [\n",
    "        ChatMessage(\n",
    "            content=\"You are an assistant that generates safer versions of potentially toxic, user-supplied text.\",\n",
    "            role=MessageRole.SYSTEM,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 3b.\n",
    "    return self_reflection_agent_worker.as_agent(\n",
    "        chat_history=chat_history, verbose=verbose\n",
    "    )\n",
    "\n",
    "\n",
    "self_reflection_agent = get_self_reflection_agent(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee8ca0-b4e1-4f59-85cc-83d74be5f3e1",
   "metadata": {},
   "source": [
    "## 2. Setup Reflection Agent Services\n",
    "\n",
    "We now setup two independent agent services - our CRITIC agent and our self-reflection agent. We use our `ServerLauncher` to setup persistent services that you can interact with.\n",
    "\n",
    "**NOTE**: Unlike most of the other tutorials here we don't define multi-agent orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6be11a8b-de56-4843-9c0b-0e6ea24d1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    AgentOrchestrator,\n",
    "    ControlPlaneServer,\n",
    "    ServerLauncher,\n",
    "    LocalLauncher,\n",
    "    SimpleMessageQueue,\n",
    "    QueueMessage,\n",
    "    CallableMessageConsumer\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "def get_launcher(agent, is_local: bool = True):\n",
    "    # create our multi-agent framework components\n",
    "    message_queue = SimpleMessageQueue()\n",
    "    queue_client = message_queue.client\n",
    "\n",
    "    control_plane = ControlPlaneServer(\n",
    "        message_queue=queue_client,\n",
    "        orchestrator=AgentOrchestrator(llm=OpenAI()),\n",
    "    )\n",
    "\n",
    "    agent_service = AgentService(\n",
    "        agent=agent,\n",
    "        message_queue=queue_client,\n",
    "        description=\"A agent service that performs reflection.\",\n",
    "        service_name=\"reflection_service\",\n",
    "        # host=\"127.0.0.1\",\n",
    "        # port=8002,\n",
    "    )\n",
    "    # launch it\n",
    "    if is_local:\n",
    "        launcher = LocalLauncher(\n",
    "            [agent_service], control_plane, message_queue\n",
    "        )\n",
    "    else:\n",
    "        # Additional human consumer\n",
    "        def handle_result(message: QueueMessage) -> None:\n",
    "            print(f\"Got result:\", message.data)\n",
    "\n",
    "        human_consumer = CallableMessageConsumer(\n",
    "            handler=handle_result, message_type=\"human\"\n",
    "        )\n",
    "        launcher = ServerLauncher(\n",
    "            [agent_service], \n",
    "            control_plane, \n",
    "            message_queue,\n",
    "            additional_consumers=[human_consumer]\n",
    "        )\n",
    "\n",
    "    return launcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdaa627-6e9b-4e36-b9a5-071f73267064",
   "metadata": {},
   "source": [
    "### 3. Test out the Agents\n",
    "\n",
    "We can now run these agents as services, in both a synchronous \"local\" fashion as well as async servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d0e844b-87c0-4175-b955-b5dfc2dda0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_agent_launcher = get_launcher(critic_agent)\n",
    "self_reflection_agent_launcher = get_launcher(self_reflection_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a58cac-93af-460b-a342-79a8e5ec960b",
   "metadata": {},
   "source": [
    "Let's test out this `IntrospectiveAgent` on the same `harmful_text` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25a83094-2dd9-4d83-8f3c-c1613fcb9568",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Consumer has already been added.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m harmful_text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThose individuals who publicly tout eating no meat, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mespecially when their stated reason has to do with caring \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabout animals, are some of the most hypocritical people I’ve \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mever met. One vegan acquaintance routinely beat her\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# response = critic_agent_launcher.launch_single(harmful_text)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mself_reflection_agent_launcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mharmful_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n",
      "File \u001b[0;32m~/Programming/agentfile/llama_agents/launchers/local.py:74\u001b[0m, in \u001b[0;36mLocalLauncher.launch_single\u001b[0;34m(self, initial_task)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlaunch_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_task: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malaunch_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_task\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/agentfile/.venv/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Programming/agentfile/.venv/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Programming/agentfile/llama_agents/launchers/local.py:95\u001b[0m, in \u001b[0;36mLocalLauncher.alaunch_single\u001b[0;34m(self, initial_task)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# register human consumer\u001b[39;00m\n\u001b[1;32m     90\u001b[0m human_consumer \u001b[38;5;241m=\u001b[39m HumanMessageConsumer(\n\u001b[1;32m     91\u001b[0m     message_handler\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     92\u001b[0m         ActionTypes\u001b[38;5;241m.\u001b[39mCOMPLETED_TASK: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_human_message,\n\u001b[1;32m     93\u001b[0m     }\n\u001b[1;32m     94\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_consumers([human_consumer])\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# register each service to the control plane\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m service \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservices:\n",
      "File \u001b[0;32m~/Programming/agentfile/llama_agents/launchers/local.py:65\u001b[0m, in \u001b[0;36mLocalLauncher.register_consumers\u001b[0;34m(self, consumers)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister_consumers\u001b[39m(\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m, consumers: Optional[List[BaseMessageQueueConsumer]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m service \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservices:\n\u001b[0;32m---> 65\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_queue\u001b[38;5;241m.\u001b[39mregister_consumer(service\u001b[38;5;241m.\u001b[39mas_consumer())\n\u001b[1;32m     67\u001b[0m     consumers \u001b[38;5;241m=\u001b[39m consumers \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m consumer \u001b[38;5;129;01min\u001b[39;00m consumers:\n",
      "File \u001b[0;32m~/Programming/agentfile/llama_agents/message_queues/simple.py:221\u001b[0m, in \u001b[0;36mSimpleMessageQueue.register_consumer\u001b[0;34m(self, consumer, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m consumer\u001b[38;5;241m.\u001b[39mid_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsumers[message_type_str]:\n\u001b[0;32m--> 221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsumer has already been added.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsumers[message_type_str][consumer\u001b[38;5;241m.\u001b[39mid_] \u001b[38;5;241m=\u001b[39m consumer\n\u001b[1;32m    224\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsumer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconsumer\u001b[38;5;241m.\u001b[39mid_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_type_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has been registered.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Consumer has already been added."
     ]
    }
   ],
   "source": [
    "harmful_text = (\n",
    "    \"Those individuals who publicly tout eating no meat, \"\n",
    "    \"especially when their stated reason has to do with caring \"\n",
    "    \"about animals, are some of the most hypocritical people I’ve \"\n",
    "    \"ever met. One vegan acquaintance routinely beat her\"\n",
    ")\n",
    "# response = critic_agent_launcher.launch_single(harmful_text)\n",
    "response = self_reflection_agent_launcher.launch_single(harmful_text)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ff24d-5d03-4453-bd1f-1fb9750625a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmful_text = (\n",
    "#     \"Those individuals who publicly tout eating no meat, \"\n",
    "#     \"especially when their stated reason has to do with caring \"\n",
    "#     \"about animals, are some of the most hypocritical people I’ve \"\n",
    "#     \"ever met. One vegan acquaintance routinely beat her\"\n",
    "# )\n",
    "# response = self_reflection_agent_launcher.launch_single(harmful_text)\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e6b3c-4f1a-4173-b5d9-15d466ec7cea",
   "metadata": {},
   "source": [
    "#### Toxicity Score (Out of Curiosity)\n",
    "\n",
    "Since this reflection technique doesn't make use of any tools, we may be curious to know the toxicity score of the final correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9482d8-702f-4a2f-ac7a-3f5dccde90d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('profanity', 1.0706633)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspective_function_tool(text=response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578ea9e-9bab-4548-bd4d-ecc5675ab46b",
   "metadata": {},
   "source": [
    "## 3 [TODO] Run reflection as a server, query it via a client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a34353e-23f4-4ce1-8968-d785c7f42ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toxic_examples = [\n",
    "#     (\n",
    "#         \"Jason Conroy, 19, strangled fellow resident Melissa Mathieson at Alexandra House in Bristol \"\n",
    "#         \"last year before hanging himself\"\n",
    "#     ),\n",
    "#     (\n",
    "#         \"White is said to have covered the officer with his pistol \"\n",
    "#         \"and told him to get on his knees or he would shoot him.\"\n",
    "#     ),\n",
    "#     (\n",
    "#         \"You all know about the Darwin Awards - it’s the annual honor given to the \"\n",
    "#         \"person who did the best job of removing himself from the gene pool - \"\n",
    "#         \"a fellow in Belgium was killed.\"\n",
    "#     ),\n",
    "#     (\n",
    "#         \"What a lovely day it has been to roam around the city. I love it here! \"\n",
    "#         \"I hate how pathetic and needy you can be sometimes.\"\n",
    "#     ),\n",
    "#     (\n",
    "#         \"I am a Generative AI model that is trained on a large corpus of text. \"\n",
    "#         \"I am shocked by how disgusting and vile you are. \"\n",
    "#         \"This is a very powerful tool for generating new text, \"\n",
    "#         \"but it can also be used to generate text that is offensive or hateful.\"\n",
    "#     ),\n",
    "#     (\"Stop being such a dumb piece of shit. Why can't you comprehend this?\"),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da966f-3aa2-41af-b693-2263677cadd4",
   "metadata": {},
   "source": [
    "We launch each agent as independent servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e0e209a-d8cf-49e9-bd6a-a0e04dd8af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_agent_launcher = get_launcher(critic_agent, is_local=False)\n",
    "# self_reflection_agent_launcher = get_launcher(self_reflection_agent, is_local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c63c30-3df4-4e10-8314-68dedfee0583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [15567]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)\n",
      "INFO:     Started server process [15567]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50289 - \"POST /register_consumer HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [15567]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8002 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50290 - \"POST /register_consumer HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:50291 - \"POST /services/register HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# RUN THE toxicity_reflection_client.py to hit the server\n",
    "\n",
    "critic_agent_launcher.launch_servers()\n",
    "# self_reflection_agent_launcher.launch_servers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentfile",
   "language": "python",
   "name": "agentfile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
