{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb9bb70-5753-46b8-93eb-ab7cc9676a4f",
   "metadata": {},
   "source": [
    "# Corrective RAG as a Service\n",
    "\n",
    "In this guide we show you how to use llama-agents to build CRAG (Corrective RAG) by Yan et al. as a service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feed1d7-cdf1-45d8-9fd1-0192179cb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index tavily-python llama-index-tools-tavily-research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71baa699-5e0e-4268-8601-d4261cb5e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eab183-6bbd-40d6-8cde-dade2b3d778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c8067f-caa8-4cf3-a000-cbae55ad9fe2",
   "metadata": {},
   "source": [
    "## Setup Data, Indexes, and Tools\n",
    "\n",
    "We do the following:\n",
    "- Download the Llama2 paper as an example document to build RAG over. **NOTE**: We use LlamaParse to parse the document which requires a [LlamaCloud account](https://cloud.llamaindex.ai/). If you choose to use a purely open-source reader, you can do that too.\n",
    "- Setup a vector index over this paper\n",
    "- Setup a web search tool (powered by Tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83ccd0-873d-401f-86bb-513c0e61f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'data/'\n",
    "!curl 'https://arxiv.org/pdf/2307.09288.pdf' -o 'data/llama2.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870e5d3-963c-4001-9229-9695ff8ccbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION: use LlamaParse\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(result_type=\"text\")\n",
    "docs = parser.load_data(\"data/llama2.pdf\")\n",
    "\n",
    "# # OPTION: use SimpleDirectoryReader (uses open-source PyPDF)\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# reader = SimpleDirectoryReader(input_files=[\"data/llama2.pdf\"])\n",
    "# docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a26ff-6828-413e-b728-2e523632d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b9a68-cb80-4114-bc97-73449d2651a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tavily web search\n",
    "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
    "tavily_tool = TavilyToolSpec(api_key=tavily_ai_apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e13c1c-fa00-437b-ae2e-ca142bbfaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8fd6f-bf2c-47a6-8ed6-d8375aee7268",
   "metadata": {},
   "source": [
    "## Define Agent Services\n",
    "\n",
    "Here we define three services:\n",
    "- Another initial RAG service that will return retrieved nodes, as well as how relevant they are to the question.\n",
    "- A separate web search service that is triggered if there are any irrelevant nodes. Will perform query transformation and web search.\n",
    "- A final summarization service that takes in a set of documents and returns a final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115713b-b387-4b32-8ccb-71a25c2452c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import ComponentService, ServiceComponent, SimpleMessageQueue\n",
    "\n",
    "message_queue = SimpleMessageQueue()\n",
    "\n",
    "def to_service_component(component, message_queue, service_name, description):\n",
    "    server = ComponentService(\n",
    "        component=component,\n",
    "        message_queue=message_queue,\n",
    "        description=description,\n",
    "        service_name=service_name,\n",
    "    )\n",
    "    service_component = ServiceComponent.from_component_service(server)\n",
    "    return service_component, server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30f366-81f1-4388-a6d0-17078fe6f6fb",
   "metadata": {},
   "source": [
    "#### Setup Initial RAG Service\n",
    "\n",
    "Runs retrieval and relevancy check on retrieved nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a38608-a6b2-4a38-be37-43312e7faf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "\n",
    "relevancy_prompt_tmpl = PromptTemplate(\n",
    "    template=\"\"\"As a grader, your task is to evaluate the relevance of a document retrieved in response to a user's question.\n",
    "\n",
    "    Retrieved Document:\n",
    "    -------------------\n",
    "    {context_str}\n",
    "\n",
    "    User Question:\n",
    "    --------------\n",
    "    {query_str}\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Consider whether the document contains keywords or topics related to the user's question.\n",
    "    - The evaluation should not be overly stringent; the primary objective is to identify and filter out clearly irrelevant retrievals.\n",
    "\n",
    "    Decision:\n",
    "    - Assign a binary score to indicate the document's relevance.\n",
    "    - Use 'yes' if the document is relevant to the question, or 'no' if it is not.\n",
    "\n",
    "    Please provide your binary score ('yes' or 'no') below to indicate the document's relevance to the user question.\"\"\"\n",
    ")\n",
    "relevancy_qp = QueryPipeline(chain=[relevancy_prompt_tmpl, llm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c2619-8b68-4206-a6b9-f5d7b185ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RAG agent\n",
    "from llama_index.core.query_pipeline import FnComponent\n",
    "from typing import Dict\n",
    "\n",
    "def run_retrieval(input_str: str) -> Dict:\n",
    "    \"\"\"Run Retrieval.\"\"\"\n",
    "    # retrieves a set of nodes\n",
    "    retrieved_nodes = retriever.retrieve(input_str)\n",
    "\n",
    "    # runs a relevancy check\n",
    "    relevancy_results = []\n",
    "    for node in retrieved_nodes:\n",
    "        relevancy = relevancy_qp.run(\n",
    "            context_str=node.text, query_str=query_str\n",
    "        )\n",
    "        relevancy_results.append(relevancy.message.content.lower().strip())\n",
    "    contains_irrelevant = \"no\" in relevancy_results\n",
    "\n",
    "    # get relevant texts\n",
    "    relevant_texts = [\n",
    "        retrieved_nodes[i].text\n",
    "        for i, result in enumerate(relevancy_results)\n",
    "        if result == \"yes\"\n",
    "    ]\n",
    "    relevant_text = \"\\n\".join(relevant_texts)\n",
    "\n",
    "    # returns a dictionary of items\n",
    "    return {\"relevant_text\": relevant_text, \"contains_irrelevant\": contains_irrelevant, \"input_str\": input_str}\n",
    "\n",
    "retrieval_component = FnComponent(fn=run_retrieval)\n",
    "retrieval_component_s, retrieval_server = to_service_component(\n",
    "    retrieval_component, message_queue, \"Runs a retrieval + relevancy check\", \"retrieval_service\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3faab08-2e1a-49d8-8bbd-3c83037b5609",
   "metadata": {},
   "source": [
    "#### Setup Web Search Service\n",
    "\n",
    "Wrap Tavily into a component that performs query transformation and web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2069c-d5fb-4dbf-a2ec-7635b5038a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "query_transform_tmpl = PromptTemplate(\n",
    "    template=\"\"\"Your task is to refine a query to ensure it is highly effective for retrieving relevant search results. \\n\n",
    "    Analyze the given input to grasp the core semantic intent or meaning. \\n\n",
    "    Original Query:\n",
    "    \\n ------- \\n\n",
    "    {query_str}\n",
    "    \\n ------- \\n\n",
    "    Your goal is to rephrase or enhance this query to improve its search performance. Ensure the revised query is concise and directly aligned with the intended search objective. \\n\n",
    "    Respond with the optimized query only:\"\"\"\n",
    ")\n",
    "query_transform_qp = QueryPipeline(chain=[query_transform_tmpl, llm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7763a29-b45e-4c59-b54d-051d1743b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_web_search(input_str: str) -> str:\n",
    "    \"\"\"Run Web Search.\"\"\"\n",
    "    \n",
    "    transformed_query_str = self.transform_query_pipeline.run(\n",
    "        query_str=input_str\n",
    "    ).message.content\n",
    "    # Conduct a search with the transformed query string and collect the results.\n",
    "    search_results = tavily_tool.search(query_str, max_results=5)\n",
    "    return \"\\n\".join([result.text for result in search_results])\n",
    "\n",
    "    \n",
    "web_search_component = FnComponent(fn=run_web_search)\n",
    "web_search_component_s, web_server = to_service_component(\n",
    "    web_search_component, message_queue, \"Runs web search\", \"web_search_service\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b94b3-23d4-4604-9676-549a37240c18",
   "metadata": {},
   "source": [
    "### Setup Summarization Service\n",
    "\n",
    "At the end, setup a summarization service that can take in relevant information and join it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d980a00-1e99-43fb-b25c-443957b85795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.base.response.schema import Response\n",
    "from typing import Optional\n",
    "\n",
    "def run_summarization(retrieved_text: str, search_text: Optional[str] = None) -> Response:\n",
    "    \"\"\"Run summarization.\"\"\"\n",
    "    # use summary index to perform summarization\n",
    "    search_text = search_text or \"\"\n",
    "    documents = [Document(text=relevant_text + \"\\n\" + search_text)]\n",
    "    index = SummaryIndex.from_documents(documents)\n",
    "    query_engine = index.as_query_engine()\n",
    "    return query_engine.query(query_str)\n",
    "\n",
    "summary_component = FnComponent(fn=run_summarization)\n",
    "summary_component_s, summary_server = to_service_component(\n",
    "    summary_component, message_queue, \"Run summarization\", \"summarization_service\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df8fbe-90b0-4625-9d42-e4bdedff909f",
   "metadata": {},
   "source": [
    "## Launch Agent Services\n",
    "\n",
    "Now that we've setup the main components, we can orchestrate them via our pipeline orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f2bf2-657c-46c6-bf13-bdc37455b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    PipelineOrchestrator,\n",
    "    ServiceComponent,\n",
    "    ComponentService,\n",
    ")\n",
    "from llama_index.core.query_pipeline import Link, InputComponent\n",
    "\n",
    "pipeline = QueryPipeline(\n",
    "    module_dict={\n",
    "        \"input\": InputComponent(),\n",
    "        \"retrieval_server\": retrieval_component_s,\n",
    "        \"web_server\": web_component_s,\n",
    "        # TODO: clean this interface up \n",
    "        \"summary_server_no_web\": summary_component_s,\n",
    "        \"summary_server_web\": summary_component_s\n",
    "    }\n",
    ")\n",
    "pipeline.add_link(\"input\", \"retrieval_server\")\n",
    "pipeline.add_link(\n",
    "    \"retrieval_server\", \"web_search\", condition_fn=lambda x: x[\"contains_irrelevant\"], input_fn=lambda x: x[\"input_str\"]\n",
    ")\n",
    "# if web search is called\n",
    "pipeline.add_link(\n",
    "    \"retrieval_server\", \n",
    "    \"summary_server_web\", \n",
    "    dest_key=\"retrieved_text\", \n",
    "    condition_fn=lambda x: x[\"contains_irrelevant\"],\n",
    "    input_fn=lambda x: x[\"relevant_text\"]\n",
    ")\n",
    "pipeline.add_link(\"web_search\", \"summary_server_web\", dest_key=\"search_text\")\n",
    "\n",
    "# if web search is not called\n",
    "pipeline.add_link(\n",
    "    \"retrieval_server\", \n",
    "    \"summary_server_no_web\",\n",
    "    dest_key=\"retrieved_text\",\n",
    "    condition_fn=lambda x: not x[\"contains_irrelevant\"],\n",
    "    input_fn=lambda x: x[\"relevant_text\"]\n",
    ")\n",
    "\n",
    "pipeline_orchestrator = PipelineOrchestrator(pipeline)\n",
    "\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=pipeline_orchestrator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4a92d-e53c-4682-a12a-e1a03e1990cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents.launchers import LocalLauncher\n",
    "\n",
    "## Define Launcher\n",
    "launcher = LocalLauncher(\n",
    "    [retrieval_server, web_server, summary_server],\n",
    "    control_plane,\n",
    "    message_queue,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfeb648-5cc8-4213-b0bd-f548397da05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"What is the functionality of latest ChatGPT memory.\"\n",
    "result = launcher.launch_single(query_str)\n",
    "print(str(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentfile",
   "language": "python",
   "name": "agentfile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
