{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9128484c-d9a4-4d79-ad7f-cdd64af91ff4",
   "metadata": {},
   "source": [
    "# Simple Pig-Latin Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7641b0-8bc6-4388-8197-b08eca72fd2c",
   "metadata": {},
   "source": [
    "## Task Eavesdropper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836056bc-9237-4818-a609-3a98cb4ff2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "685198c9-e028-4649-a2d0-21ee1515529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a76db1-c106-4161-81a4-f317d8e9044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents.message_queues.rabbitmq import RabbitMQMessageQueue\n",
    "from multi_agent_system.utils import load_from_env\n",
    "\n",
    "message_queue_host = load_from_env(\"RABBITMQ_HOST\")\n",
    "message_queue_port = load_from_env(\"RABBITMQ_NODE_PORT\")\n",
    "message_queue_username = load_from_env(\"RABBITMQ_DEFAULT_USER\")\n",
    "message_queue_password = load_from_env(\"RABBITMQ_DEFAULT_PASS\")\n",
    "\n",
    "message_queue = RabbitMQMessageQueue(\n",
    "    url=f\"amqp://{message_queue_username}:{message_queue_password}@{message_queue_host}:{message_queue_port}/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1a0c8b-e1a9-4679-b0bf-1f5cd5f1f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents.types import TASK_CONSUMER_NAME\n",
    "from llama_agents.message_consumers.base import (\n",
    "    BaseMessageQueueConsumer,\n",
    "    StartConsumingCallable,\n",
    ")\n",
    "from llama_agents import QueueMessage, CallableMessageConsumer\n",
    "from queue import Queue, Empty\n",
    "from pydantic import PrivateAttr\n",
    "from typing import Any\n",
    "\n",
    "task_message_queue = Queue()\n",
    "\n",
    "def task_message_queue_producer_closure(queue: Queue):\n",
    "    \"\"\"Consumer of the multi-agent system, but producer of stream.\"\"\"\n",
    "    def process_message(message: QueueMessage, **kwargs: Any):\n",
    "        queue.put(message)\n",
    "\n",
    "    return process_message\n",
    "\n",
    "message_producer = task_message_queue_producer_closure(queue=task_message_queue)\n",
    "\n",
    "task_message_eavesdropper = CallableMessageConsumer(\n",
    "    message_type=TASK_CONSUMER_NAME,\n",
    "    handler=message_producer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610ea960-2003-4d1b-9958-c756ffb27199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_agents.message_queues.rabbitmq - Registered consumer 69f4a4da-d290-427f-bdb8-cf75cfd1c6bc: task_eavesdropper\n"
     ]
    }
   ],
   "source": [
    "task_eavesdropper_start_consuming = await message_queue.register_consumer(task_message_eavesdropper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273cd826-01a5-41d2-b642-4b7625c8e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "_ = asyncio.create_task(task_eavesdropper_start_consuming())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffea2bf-648f-404c-8119-0c382421ae79",
   "metadata": {},
   "source": [
    "## Task Progress Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4579e98-a84f-40fc-bcae-9732355c56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import deque\n",
    "import json\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "# figure, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "\n",
    "class TaskProgress:\n",
    "    def __init__(self, names: List[str]):\n",
    "        self.queues = {name: deque() for name in names}\n",
    "        self.task_position = {}\n",
    "        self.name_position = {name: ix for ix, name in enumerate(names)}\n",
    "\n",
    "\n",
    "    def reposition_task(self, task_id: str, name: str, text: str):\n",
    "        if task_id in self.task_position:\n",
    "            old_owner = self.task_position.pop(task_id)\n",
    "            self.queues[old_owner].popleft()  # should be by order, right?\n",
    "\n",
    "        # assign new position\n",
    "        self.task_position[task_id] = name\n",
    "        self.queues[name].append((task_id, text))\n",
    "\n",
    "    def write_plot_data(self):        \n",
    "        # prepare data\n",
    "        data = []\n",
    "        for name, queue in self.queues.items():\n",
    "            for ix, (task_id, text) in enumerate(queue):\n",
    "                data.append((self.name_position[name], ix, text))\n",
    "\n",
    "        with open(\"sim_data.json\", \"w\") as f:\n",
    "            json.dump({\n",
    "                \"xs\": [el[0] for el in data],\n",
    "                \"ys\": [el[1] for el in data],\n",
    "                \"texts\": [el[2] for el in data]\n",
    "            }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99b2180-a5cc-49ca-9b6a-16a03289aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(queue, event):\n",
    "    \"\"\"Consumes the TaskResult and TaskDef messages.\"\"\"\n",
    "    task_progress = TaskProgress(names=[\"remove_ay_agent\",\"correct_first_character_agent\", \"human\"])\n",
    "    while not event.is_set():\n",
    "        message = queue.get()\n",
    "        name = message.data[\"original_type\"]\n",
    "        if \"input\" in message.data:\n",
    "            text = message.data[\"input\"]\n",
    "        elif \"result\" in message.data:\n",
    "            text = message.data[\"result\"]\n",
    "        else:\n",
    "            text = \"\"\n",
    "        task_id = message.data[\"task_id\"]\n",
    "        if name in task_progress.queues:\n",
    "            task_progress.reposition_task(task_id, name, text)\n",
    "            task_progress.write_plot_data()\n",
    "            # print(task_progress.queues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57390667-acd3-4e56-abbc-ff37699f2e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "event = threading.Event()\n",
    "thread = threading.Thread(target=consumer, args=(task_message_queue, event))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2eeb45-097a-4723-9f32-e73ec0770590",
   "metadata": {},
   "source": [
    "## Send Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbcf9da6-c55a-4c0d-9e8e-1e20f7459b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import LlamaAgentsClient\n",
    "\n",
    "client = LlamaAgentsClient(\"http://0.0.0.0:8001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34215265-5d78-4765-8a85-d49064035395",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = client.create_task(\"Please decode the following input: 'ellohay owhay reaay ouyay'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49b2d6ea-1dc1-496a-9a88-91bb584671e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "llama_agents.types.TaskResult() argument after ** must be a mapping, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m task_result \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_task_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(task_result\u001b[38;5;241m.\u001b[39mresult)\n",
      "File \u001b[0;32m~/Projects/llama-agents/llama_agents/client/sync_client.py:135\u001b[0m, in \u001b[0;36mLlamaAgentsClient.get_task_result\u001b[0;34m(self, task_id)\u001b[0m\n\u001b[1;32m    132\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_task(task_id)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TaskResult(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtask\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not have a result yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: llama_agents.types.TaskResult() argument after ** must be a mapping, not NoneType"
     ]
    }
   ],
   "source": [
    "task_result = client.get_task_result(task_id)\n",
    "print(task_result.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f199bb1c-732e-4aff-a1a5-10b4c738cf99",
   "metadata": {},
   "source": [
    "### Send A Bunch of Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596644a0-6c08-4c1a-97c0-ab966c465eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import LlamaAgentsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f168afc1-d49e-418c-aabf-7f38f1dca23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pig_latin(text: str):\n",
    "    tokens = text.lower().split()\n",
    "    tmp = []\n",
    "    for token in tokens:\n",
    "        token = token[1:] + token[0] + 'ay'\n",
    "        tmp.append(token)\n",
    "\n",
    "    return ' '.join(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f78009e0-eb26-4243-a1f3-d8342f46b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "\n",
    "client = LlamaAgentsClient(\"http://0.0.0.0:8001\")\n",
    "llm = OpenAI(\"gpt-4o\", temperature=1)\n",
    "\n",
    "async def send_new_task():\n",
    "    seed = random.random()\n",
    "    response = await llm.acomplete(f\"({seed}) Provide a 3 to 5 word phrase. Don't include any punctuation.\")\n",
    "    task = pig_latin(response.text)\n",
    "    print(f\"text: {response.text}, task: {task}\")\n",
    "    client.create_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ff35739-ead4-473f-9ef2-2ce240287c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Floating through serene skies, task: loatingfay hroughtay erenesay kiessay\n"
     ]
    }
   ],
   "source": [
    "await send_new_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79e7b60e-d93a-4b14-90f7-7bb7a00eff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "async def generate_tasks(n = 5):\n",
    "    try:\n",
    "        while True:\n",
    "            interarrival_time = np.random.exponential(0.25)\n",
    "            await asyncio.sleep(interarrival_time)\n",
    "            await send_new_task()\n",
    "    except (CancelledError, KeyboardInterrupt):\n",
    "        print(\"Shutting down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ee165b3-e4b2-4fe7-857b-b1be3a9addb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Whisper of the night, task: hisperway foay hetay ightnay\n",
      "text: Whispering winds of change, task: hisperingway indsway foay hangecay\n",
      "text: Embrace the journey ahead, task: mbraceeay hetay ourneyjay headaay\n",
      "text: Whispering winds through trees, task: hisperingway indsway hroughtay reestay\n",
      "text: Innovate for a better tomorrow, task: nnovateiay orfay aay etterbay omorrowtay\n",
      "text: Whispering winds of change, task: hisperingway indsway foay hangecay\n",
      "text: Whispering winds of autumn, task: hisperingway indsway foay utumnaay\n",
      "text: Serene dawn breaking horizon, task: erenesay awnday reakingbay orizonhay\n",
      "text: Whisper of the wind, task: hisperway foay hetay indway\n",
      "text: Skyline over the horizon, task: kylinesay veroay hetay orizonhay\n",
      "text: Whispers of the past, task: hispersway foay hetay astpay\n",
      "text: Whispering across still waters, task: hisperingway crossaay tillsay atersway\n",
      "text: Whispering winds through trees, task: hisperingway indsway hroughtay reestay\n",
      "text: Whispers of autumn leaves, task: hispersway foay utumnaay eaveslay\n",
      "text: Whispering winds beneath moonlight, task: hisperingway indsway eneathbay oonlightmay\n",
      "text: Shimmering in the twilight, task: himmeringsay niay hetay wilighttay\n",
      "text: Whispering winds carry secrets, task: hisperingway indsway arrycay ecretssay\n",
      "text: Whisper of the wind, task: hisperway foay hetay indway\n",
      "text: Sure, here's a phrase:\n",
      "\n",
      "blissful moments in nature, task: ure,say ere'shay aay hrase:pay lissfulbay omentsmay niay aturenay\n",
      "text: Limitless creativity knows no bounds, task: imitlesslay reativitycay nowskay onay oundsbay\n",
      "text: Whispering winds through trees, task: hisperingway indsway hroughtay reestay\n",
      "text: Sure thing: \"Whispers in the wind\", task: uresay hing:tay whispers\"ay niay hetay ind\"way\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m generate_tasks()\n",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m, in \u001b[0;36mgenerate_tasks\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         interarrival_time \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexponential(\u001b[38;5;241m0.25\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(interarrival_time)\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m send_new_task()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/asyncio/tasks.py:605\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    601\u001b[0m h \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mcall_later(delay,\n\u001b[1;32m    602\u001b[0m                     futures\u001b[38;5;241m.\u001b[39m_set_result_unless_cancelled,\n\u001b[1;32m    603\u001b[0m                     future, result)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m     h\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/asyncio/futures.py:285\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "await generate_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892e412-130d-49d2-82e6-377dea6d56c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task-simulation",
   "language": "python",
   "name": "task-simulation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
